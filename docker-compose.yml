services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: tutor-ai-backend:ready
    container_name: tutor-ai-backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./backend/.env:/app/.env
      - rag_data:/app/rag
      - apt_cache:/var/cache/apt
      - apt_lib_cache:/var/lib/apt
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - PYTHONPATH=/app
      - CORS_ORIGINS=http://localhost:5000,http://127.0.0.1:5000
      - REDIS_URL=redis://redis:6379
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    entrypoint: ["/opt/venv/bin/uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: tutor-ai-frontend:latest
    container_name: tutor-ai-frontend
    ports:
      - "5000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NODE_ENV=production
      - NEXT_TELEMETRY_DISABLED=1
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Redis for caching
  redis:
    image: redis:7-alpine
    container_name: tutor-ai-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Ollama removed - using Z.AI instead

volumes:
  redis_data:
    driver: local
  rag_data:
    driver: local
  apt_cache:
    driver: local
  apt_lib_cache:
    driver: local

networks:
  default:
    name: tutor-ai-network
    driver: bridge

# Development profile
x-dev-volumes: &dev-volumes
  - ./frontend:/app
  - /app/node_modules
  - /app/.next
  - ./backend:/app

# Development override
x-dev-frontend: &dev-frontend
  environment:
    - NEXT_PUBLIC_API_URL=http://localhost:8000
    - NODE_ENV=development
  volumes: *dev-volumes
