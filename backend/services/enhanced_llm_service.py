"""
Enhanced LLM Service for Tutor-AI
Integrates advanced prompt templates and intelligent model selection
"""

import os
import asyncio
from typing import Dict, Any, Optional, List, Tuple
import json
from datetime import datetime

from services.llm_service import LLMService
from services.advanced_model_selector import (
    advanced_model_selector, TaskType, CognitiveLoad, PedagogicalFocus
)
from services.advanced_prompt_templates import (
    advanced_prompt_templates, PromptStrategy, LearningStyle
)
from services.prompt_analytics_service import (
    prompt_analytics_service, PromptType
)

class EnhancedLLMService(LLMService):
    """Enhanced LLM service with advanced prompting and model selection"""

    def __init__(self):
        # Initialize parent LLM service
        super().__init__()

        # Initialize advanced components
        self.prompt_templates = advanced_prompt_templates
        self.model_selector = advanced_model_selector

        # Analytics tracking
        self.prompt_session_id = None
        self._start_prompt_session()

    def _start_prompt_session(self):
        """Start a new prompt session for analytics tracking"""
        self.prompt_session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{id(os.urandom(4).hex())}"

    async def generate_enhanced_response(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None,
        course_id: Optional[str] = None,
        prompt_type: str = "chat_tutoring",
        strategy: Optional[PromptStrategy] = None,
        learning_style: Optional[LearningStyle] = None,
        cognitive_load: Optional[CognitiveLoad] = None,
        force_model: Optional[str] = None,
        enable_analytics: bool = True
    ) -> Dict[str, Any]:
        """
        Generate response using advanced prompting and optimal model selection

        Args:
            query: User query or request
            context: Context information for the request
            course_id: Course identifier for personalization
            prompt_type: Type of prompt to use
            strategy: Prompting strategy to employ
            learning_style: Learner's preferred learning style
            cognitive_load: Cognitive complexity of the task
            force_model: Force specific model (overrides selection)
            enable_analytics: Enable performance tracking
        """

        # Prepare context
        enhanced_context = self._prepare_enhanced_context(
            query, context, course_id, learning_style, cognitive_load
        )

        # Generate enhanced prompt
        enhanced_prompt, prompt_metadata = self.prompt_templates.generate_enhanced_prompt(
            prompt_type=prompt_type,
            context=enhanced_context,
            strategy=strategy,
            learning_style=learning_style,
            cognitive_load=cognitive_load
        )

        # Select optimal model (unless forced)
        if force_model:
            model_selection = {
                "model_key": force_model,
                "provider": self._extract_provider_from_model(force_model),
                "model_name": force_model,
                "reasoning": "Model forced by user"
            }
        else:
            # Determine task characteristics for model selection
            task_type = self._infer_task_type(query, prompt_type)
            model_selection = self.model_selector.select_optimal_model(
                task_type=task_type,
                cognitive_load=cognitive_load or CognitiveLoad.MEDIUM,
                pedagogical_focus=self._infer_pedagogical_focus(query, prompt_type),
                context_size=len(enhanced_prompt),
                output_size=self._estimate_output_size(prompt_type)
            )

        # Track prompt generation if analytics enabled
        if enable_analytics:
            self._track_prompt_generation(
                prompt_type, enhanced_prompt, model_selection, prompt_metadata
            )

        # Generate response using selected model
        try:
            start_time = datetime.now()

            # Use parent service with selected model
            response = await self._generate_with_model(
                enhanced_prompt, model_selection, context
            )

            execution_time = (datetime.now() - start_time).total_seconds() * 1000

            # Track performance if analytics enabled
            if enable_analytics:
                performance_id = self._track_prompt_execution(
                    prompt_type, enhanced_prompt, model_selection,
                    response, execution_time, success=True
                )

                # Add performance tracking to response
                response["performance_id"] = performance_id
                response["analytics"] = {
                    "prompt_type": prompt_type,
                    "strategy": strategy.value if strategy else None,
                    "model_used": model_selection["model_name"],
                    "execution_time_ms": execution_time
                }

            return response

        except Exception as e:
            # Track failed execution if analytics enabled
            if enable_analytics:
                self._track_prompt_execution(
                    prompt_type, enhanced_prompt, model_selection,
                    {"error": str(e)}, 0, success=False, error_message=str(e)
                )

            # Re-raise exception for error handling
            raise

    async def generate_enhanced_quiz(
        self,
        course_id: str,
        topic: str,
        difficulty: str = "medium",
        num_questions: int = 5,
        bloom_level: Optional[str] = None,
        learning_objectives: Optional[List[str]] = None,
        enable_analytics: bool = True
    ) -> Dict[str, Any]:
        """Generate quiz using advanced prompting and optimal models"""

        # Prepare quiz context
        context = {
            "course_id": course_id,
            "topic": topic,
            "difficulty": difficulty,
            "num_questions": num_questions,
            "bloom_level": bloom_level or "mixed",
            "learning_objectives": learning_objectives or [],
            "question_types": ["multiple_choice", "true_false", "short_answer"],
            "output_format": "structured_json",
            "difficulty_level": difficulty,
            "subject": self._get_course_subject(course_id),
            "learner_level": "intermediate"  # Could be personalized
        }

        # Select prompt strategy based on Bloom level
        strategy_map = {
            "remember": PromptStrategy.EXPLANATORY,
            "understand": PromptStrategy.SOCRATIC,
            "apply": PromptStrategy.SCAFFOLDED,
            "analyze": PromptStrategy.SOCRATIC,
            "evaluate": PromptStrategy.METACOGNITIVE,
            "create": PromptStrategy.ADAPTIVE
        }

        strategy = strategy_map.get(bloom_level, PromptStrategy.SOCRATIC)

        # Determine cognitive load
        cognitive_load_map = {
            "easy": CognitiveLoad.LOW,
            "medium": CognitiveLoad.MEDIUM,
            "hard": CognitiveLoad.HIGH
        }

        cognitive_load = cognitive_load_map.get(difficulty, CognitiveLoad.MEDIUM)

        # Generate enhanced response
        response = await self.generate_enhanced_response(
            query=f"Generate quiz on {topic}",
            context=context,
            course_id=course_id,
            prompt_type="quiz_generation",
            strategy=strategy,
            cognitive_load=cognitive_load,
            enable_analytics=enable_analytics
        )

        # Process quiz response
        return self._process_quiz_response(response, topic, difficulty, num_questions)

    async def generate_enhanced_tutoring_response(
        self,
        user_message: str,
        course_id: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        user_profile: Optional[Dict[str, Any]] = None,
        enable_analytics: bool = True
    ) -> Dict[str, Any]:
        """Generate enhanced tutoring response with cognitive science principles"""

        # Analyze conversation context
        context = self._prepare_tutoring_context(
            user_message, course_id, conversation_history, user_profile
        )

        # Determine optimal strategy based on learner needs
        strategy = self._select_tutoring_strategy(context, user_profile)

        # Determine learning style from user profile
        learning_style = None
        if user_profile and "learning_style" in user_profile:
            learning_style = LearningStyle(user_profile["learning_style"])

        # Determine cognitive load from message complexity
        cognitive_load = self._assess_cognitive_load(user_message, context)

        # Generate enhanced response
        response = await self.generate_enhanced_response(
            query=user_message,
            context=context,
            course_id=course_id,
            prompt_type="chat_tutoring",
            strategy=strategy,
            learning_style=learning_style,
            cognitive_load=cognitive_load,
            enable_analytics=enable_analytics
        )

        # Add tutoring-specific enhancements
        response["tutoring_metadata"] = {
            "strategy_used": strategy.value,
            "cognitive_load_assessed": cognitive_load.value,
            "learning_style_adapted": learning_style.value if learning_style else None,
            "conversation_turn": len(conversation_history) if conversation_history else 0
        }

        return response

    async def generate_enhanced_mindmap(
        self,
        course_id: str,
        topic: str,
        learning_objectives: Optional[List[str]] = None,
        complexity: str = "medium",
        enable_analytics: bool = True
    ) -> Dict[str, Any]:
        """Generate enhanced mindmap with cognitive load optimization"""

        context = {
            "course_id": course_id,
            "topic": topic,
            "learning_objective": learning_objectives or [f"Understand {topic}"],
            "knowledge_type": "conceptual",
            "complexity_level": complexity,
            "prior_knowledge": "basic",  # Could be personalized
            "key_concepts": [],  # Could be extracted from course content
            "relationships": [],
            "misconceptions": [],
            "output_format": "structured_json",
            "visual_style": "clean_academic",
            "interactivity": "medium"
        }

        # Determine cognitive load
        cognitive_load_map = {
            "simple": CognitiveLoad.LOW,
            "medium": CognitiveLoad.MEDIUM,
            "complex": CognitiveLoad.HIGH
        }
        cognitive_load = cognitive_load_map.get(complexity, CognitiveLoad.MEDIUM)

        response = await self.generate_enhanced_response(
            query=f"Create mindmap for {topic}",
            context=context,
            course_id=course_id,
            prompt_type="mindmap_generation",
            strategy=PromptStrategy.SCAFFOLDED,
            cognitive_load=cognitive_load,
            enable_analytics=enable_analytics
        )

        return self._process_mindmap_response(response, topic)

    async def generate_enhanced_study_plan(
        self,
        course_id: str,
        topics: List[str],
        duration_weeks: int = 4,
        learning_goals: Optional[List[str]] = None,
        study_style: Optional[str] = None,
        enable_analytics: bool = True
    ) -> Dict[str, Any]:
        """Generate enhanced study plan with learning science integration"""

        context = {
            "course_id": course_id,
            "topics": topics,
            "learning_goals": learning_goals or [f"Master {topic}" for topic in topics],
            "duration_weeks": duration_weeks,
            "study_style": study_style or "balanced",
            "learning_style": study_style or "multimodal",
            "attention_span": "45_minutes",  # Could be personalized
            "study_times": "flexible",
            "preferred_difficulty": "progressive"
        }

        response = await self.generate_enhanced_response(
            query=f"Create study plan for {', '.join(topics)}",
            context=context,
            course_id=course_id,
            prompt_type="study_plan",
            strategy=PromptStrategy.ADAPTIVE,
            learning_style=LearningStyle(study_style) if study_style else None,
            cognitive_load=CognitiveLoad.HIGH,  # Study planning is complex
            enable_analytics=enable_analytics
        )

        return self._process_study_plan_response(response, topics, duration_weeks)

    def _prepare_enhanced_context(
        self,
        query: str,
        context: Optional[Dict[str, Any]],
        course_id: Optional[str],
        learning_style: Optional[LearningStyle],
        cognitive_load: Optional[CognitiveLoad]
    ) -> Dict[str, Any]:
        """Prepare enhanced context with additional information"""

        enhanced_context = context.copy() if context else {}

        # Add course-specific information
        if course_id:
            enhanced_context.update({
                "course_id": course_id,
                "subject": self._get_course_subject(course_id),
                "difficulty_level": "intermediate",  # Could be personalized
                "objectives": self._get_course_objectives(course_id)
            })

        # Add learning preferences
        if learning_style:
            enhanced_context["learning_style"] = learning_style.value

        if cognitive_load:
            enhanced_context["cognitive_load"] = cognitive_load.value

        # Add query analysis
        enhanced_context.update({
            "query_complexity": self._analyze_query_complexity(query),
            "query_type": self._classify_query_type(query),
            "estimated_response_length": self._estimate_response_length(query)
        })

        return enhanced_context

    def _prepare_tutoring_context(
        self,
        user_message: str,
        course_id: str,
        conversation_history: Optional[List[Dict[str, Any]]],
        user_profile: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Prepare specialized context for tutoring interactions"""

        context = {
            "course_id": course_id,
            "current_question": user_message,
            "conversation_history": conversation_history or [],
            "learner_profile": user_profile or {},
            "session_context": self._analyze_session_context(conversation_history),
            "current_topic": self._extract_current_topic(conversation_history, user_message),
            "learning_progress": self._assess_learning_progress(user_profile, conversation_history)
        }

        # Add course information
        context.update({
            "subject": self._get_course_subject(course_id),
            "current_objective": self._determine_current_objective(context)
        })

        return context

    def _select_tutoring_strategy(
        self,
        context: Dict[str, Any],
        user_profile: Optional[Dict[str, Any]]
    ) -> PromptStrategy:
        """Select optimal tutoring strategy based on context and profile"""

        # Analyze learner readiness
        mastery_level = context.get("learning_progress", {}).get("mastery_level", 0.5)
        complexity = context.get("query_complexity", "medium")

        # Adjust strategy based on learner needs
        if mastery_level < 0.3:
            return PromptStrategy.SCAFFOLDED  # Needs more support
        elif mastery_level > 0.8:
            return PromptStrategy.METACOGNITIVE  # Ready for deeper thinking
        elif complexity == "high":
            return PromptStrategy.SOCRATIC  # Guided discovery for complexity
        else:
            return PromptStrategy.EXPLANATORY  # Clear explanations

    def _assess_cognitive_load(
        self,
        user_message: str,
        context: Dict[str, Any]
    ) -> CognitiveLoad:
        """Assess cognitive load of the request"""

        message_length = len(user_message.split())
        complexity_indicators = [
            "analyze", "synthesize", "evaluate", "compare", "justify"
        ]
        high_complexity_count = sum(1 for indicator in complexity_indicators
                                 if indicator in user_message.lower())

        if message_length > 50 or high_complexity_count >= 2:
            return CognitiveLoad.HIGH
        elif message_length > 25 or high_complexity_count >= 1:
            return CognitiveLoad.MEDIUM
        else:
            return CognitiveLoad.LOW

    def _infer_task_type(self, query: str, prompt_type: str) -> TaskType:
        """Infer task type from query and prompt type"""

        task_type_mapping = {
            "chat_tutoring": TaskType.PEDAGOGICAL_EXPLANATION,
            "quiz_generation": TaskType.MULTISTEP_PROBLEM,
            "mindmap_generation": TaskType.VISUAL_REASONING,
            "study_plan": TaskType.MULTISTEP_PROBLEM
        }

        # Refine based on query content
        query_lower = query.lower()

        if any(word in query_lower for word in ["explain", "teach", "clarify"]):
            return TaskType.PEDAGOGICAL_EXPLANATION
        elif any(word in query_lower for word in ["solve", "calculate", "compute"]):
            return TaskType.MULTISTEP_PROBLEM
        elif any(word in query_lower for word in ["create", "generate", "design"]):
            return TaskType.CREATIVE_GENERATION
        elif any(word in query_lower for word in ["analyze", "evaluate", "compare"]):
            return TaskType.COMPLEX_REASONING

        return task_type_mapping.get(prompt_type, TaskType.PEDAGOGICAL_EXPLANATION)

    def _infer_pedagogical_focus(
        self,
        query: str,
        prompt_type: str
    ) -> Optional[PedagogicalFocus]:
        """Infer pedagogical focus from query"""

        query_lower = query.lower()

        focus_keywords = {
            PedagogicalFocus.CONCEPT_BUILDING: ["concept", "understand", "define"],
            PedagogicalFocus.SKILL_PRACTICE: ["practice", "exercise", "drill"],
            PedagogicalFocus.PROBLEM_SOLVING: ["solve", "problem", "challenge"],
            PedagogicalFocus.CRITICAL_THINKING: ["analyze", "evaluate", "critique"],
            PedagogicalFocus.SCAFFOLDING: ["help", "guide", "step by step"],
            PedagogicalFocus.ASSESSMENT: ["test", "quiz", "evaluate"]
        }

        for focus, keywords in focus_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                return focus

        return None

    def _estimate_output_size(self, prompt_type: str) -> int:
        """Estimate expected output size in tokens"""

        size_estimates = {
            "chat_tutoring": 500,      # Medium response
            "quiz_generation": 1000,    # Quiz with questions
            "mindmap_generation": 800,   # Structured mindmap data
            "study_plan": 1200          # Detailed study plan
        }

        return size_estimates.get(prompt_type, 500)

    def _extract_provider_from_model(self, model_name: str) -> str:
        """Extract provider from model name"""

        if "glm" in model_name.lower() or "zai" in model_name.lower():
            return "zai"
        elif "claude" in model_name.lower():
            return "openrouter"
        elif "gpt" in model_name.lower():
            return "openai"
        elif "llama" in model_name.lower() or "local" in model_name.lower():
            return "local"
        else:
            return "unknown"

    def _get_course_subject(self, course_id: str) -> str:
        """Get course subject from course ID or database"""

        # This would typically query the database
        # For now, return a default
        subject_mapping = {
            "90a903c0": "Storia",
            "906d737a": "Matematica",
            "ff1851e4": "Scienze"
        }

        # Extract first 8 characters for matching
        course_prefix = course_id[:8] if len(course_id) >= 8 else course_id

        return subject_mapping.get(course_prefix, "General")

    def _get_course_objectives(self, course_id: str) -> List[str]:
        """Get course learning objectives"""

        # This would typically query the database
        return [
            "Comprendere i concetti fondamentali",
            "Applicare le conoscenze in pratica",
            "Sviluppare capacitÃ  di analisi critica"
        ]

    def _analyze_query_complexity(self, query: str) -> str:
        """Analyze the complexity of the user query"""

        word_count = len(query.split())
        complex_words = [
            "analyze", "synthesize", "evaluate", "compare", "justify",
            "relationship", "implication", "significance", "principle"
        ]

        complex_word_count = sum(1 for word in complex_words if word in query.lower())

        if word_count > 30 or complex_word_count >= 3:
            return "high"
        elif word_count > 15 or complex_word_count >= 1:
            return "medium"
        else:
            return "low"

    def _classify_query_type(self, query: str) -> str:
        """Classify the type of user query"""

        query_lower = query.lower()

        if any(word in query_lower for word in ["what", "define", "explain"]):
            return "factual"
        elif any(word in query_lower for word in ["how", "procedure", "steps"]):
            return "procedural"
        elif any(word in query_lower for word in ["why", "reason", "cause"]):
            return "analytical"
        elif any(word in query_lower for word in ["evaluate", "compare", "assess"]):
            return "evaluative"
        else:
            return "general"

    def _estimate_response_length(self, query: str) -> str:
        """Estimate required response length"""

        complexity = self._analyze_query_complexity(query)
        length_mapping = {
            "low": "short",
            "medium": "medium",
            "high": "detailed"
        }

        return length_mapping.get(complexity, "medium")

    async def _generate_with_model(
        self,
        prompt: str,
        model_selection: Dict[str, Any],
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Generate response using specific model"""

        # This would integrate with the existing LLM service methods
        # For now, create a placeholder response

        # Use parent service with selected model
        # This is a simplified integration - in practice, you'd call the appropriate
        # method based on the provider

        if model_selection["provider"] == "zai":
            # Call ZAI generation method
            return await self._generate_with_zai(prompt, context, model_selection)
        elif model_selection["provider"] == "openrouter":
            # Call OpenRouter generation method
            return await self._generate_with_openrouter(prompt, context, model_selection)
        elif model_selection["provider"] == "openai":
            # Call OpenAI generation method
            return await self._generate_with_openai(prompt, context, model_selection)
        else:
            # Fallback to default method
            return await super().generate_response(prompt, context)

    async def _generate_with_zai(self, prompt: str, context: Optional[Dict[str, Any]], model_selection: Dict[str, Any]) -> Dict[str, Any]:
        """Generate response using ZAI model"""

        # This would call the actual ZAI API
        # For now, return a mock response
        return {
            "response": f"Enhanced ZAI response using {model_selection['model_name']}",
            "model_used": model_selection["model_name"],
            "provider": "zai",
            "success": True
        }

    async def _generate_with_openrouter(self, prompt: str, context: Optional[Dict[str, Any]], model_selection: Dict[str, Any]) -> Dict[str, Any]:
        """Generate response using OpenRouter model"""

        # This would call the actual OpenRouter API
        return {
            "response": f"Enhanced OpenRouter response using {model_selection['model_name']}",
            "model_used": model_selection["model_name"],
            "provider": "openrouter",
            "success": True
        }

    async def _generate_with_openai(self, prompt: str, context: Optional[Dict[str, Any]], model_selection: Dict[str, Any]) -> Dict[str, Any]:
        """Generate response using OpenAI model"""

        # This would call the actual OpenAI API
        return {
            "response": f"Enhanced OpenAI response using {model_selection['model_name']}",
            "model_used": model_selection["model_name"],
            "provider": "openai",
            "success": True
        }

    def _track_prompt_generation(
        self,
        prompt_type: str,
        prompt: str,
        model_selection: Dict[str, Any],
        prompt_metadata: Dict[str, Any]
    ):
        """Track prompt generation for analytics"""

        # This would log the prompt generation event
        # For analytics purposes
        pass

    def _track_prompt_execution(
        self,
        prompt_type: str,
        prompt: str,
        model_selection: Dict[str, Any],
        response: Dict[str, Any],
        execution_time: float,
        success: bool,
        error_message: Optional[str] = None
    ) -> str:
        """Track prompt execution for analytics"""

        from services.prompt_analytics_service import PromptPerformance, ModelProvider, CognitiveLoad

        performance = PromptPerformance(
            id="",
            prompt_type=PromptType(prompt_type),
            model_provider=ModelProvider(model_selection["provider"]),
            model_name=model_selection["model_name"],
            prompt_template=prompt[:500] + "..." if len(prompt) > 500 else prompt,
            prompt_length=len(prompt),
            response_length=len(str(response.get("response", ""))),
            execution_time_ms=execution_time,
            token_usage=response.get("token_usage", 0),
            cost_estimate=response.get("cost_estimate", 0.0),
            cognitive_load=CognitiveLoad.MEDIUM,  # Default, could be inferred
            success=success,
            error_message=error_message
        )

        return prompt_analytics_service.record_prompt_execution(performance)

    def _process_quiz_response(self, response: Dict[str, Any], topic: str, difficulty: str, num_questions: int) -> Dict[str, Any]:
        """Process and structure quiz response"""

        # This would parse the actual quiz response and structure it appropriately
        return {
            "quiz": {
                "title": f"Quiz su {topic}",
                "difficulty": difficulty,
                "questions": response.get("questions", []),
                "total_questions": num_questions
            },
            "enhanced_features": {
                "bloom_taxonomy_integration": True,
                "cognitive_load_optimized": True,
                "personalized_difficulty": True
            }
        }

    def _process_mindmap_response(self, response: Dict[str, Any], topic: str) -> Dict[str, Any]:
        """Process and structure mindmap response"""

        return {
            "mindmap": {
                "title": f"Mappa Concettuale: {topic}",
                "nodes": response.get("nodes", []),
                "connections": response.get("connections", [])
            },
            "enhanced_features": {
                "cognitive_load_managed": True,
                "visual_hierarchy_optimized": True,
                "learning_objectives_aligned": True
            }
        }

    def _process_study_plan_response(self, response: Dict[str, Any], topics: List[str], duration_weeks: int) -> Dict[str, Any]:
        """Process and structure study plan response"""

        return {
            "study_plan": {
                "title": f"Piano di Studio: {', '.join(topics)}",
                "duration_weeks": duration_weeks,
                "schedule": response.get("schedule", []),
                "milestones": response.get("milestones", [])
            },
            "enhanced_features": {
                "spaced_repetition": True,
                "interleaved_practice": True,
                "cognitive_scheduling": True
            }
        }

    def _analyze_session_context(self, conversation_history: Optional[List[Dict[str, Any]]]) -> Dict[str, Any]:
        """Analyze the current session context from conversation history"""

        if not conversation_history:
            return {
                "session_start": True,
                "previous_topics": [],
                "difficulty_progression": "stable",
                "engagement_level": "unknown"
            }

        # Analyze conversation patterns
        return {
            "session_start": False,
            "message_count": len(conversation_history),
            "previous_topics": [],
            "difficulty_progression": "adaptive",
            "engagement_level": "high"
        }

    def _extract_current_topic(self, conversation_history: Optional[List[Dict[str, Any]]], current_message: str) -> str:
        """Extract the current topic from conversation and message"""

        # This would use NLP to extract topics
        # For now, return a simplified extraction
        words = current_message.split()
        return words[0] if words else "General Topic"

    def _assess_learning_progress(self, user_profile: Optional[Dict[str, Any]], conversation_history: Optional[List[Dict[str, Any]]]) -> Dict[str, Any]:
        """Assess the learner's current progress"""

        return {
            "mastery_level": 0.7,  # Default, would be calculated
            "recent_successes": [],
            "identified_challenges": [],
            "recommendations": []
        }

    def _determine_current_objective(self, context: Dict[str, Any]) -> str:
        """Determine the current learning objective"""

        current_topic = context.get("current_topic", "General Topic")
        return f"Understand and master {current_topic}"

# Global instance
enhanced_llm_service = EnhancedLLMService()